                    Live System Integration Tests

Rationale
---------

While unit tests and Go integration tests catch most bugs, some issues only 
surface when tdh interacts with a real system environment. Dotfiles management 
involves complex interactions with:

• User home directories with existing files
• Shell initialization sequences
• Package managers like Homebrew
• Symlink operations across different file systems
• Environment variable propagation

The live testing system provides a sandboxed environment that simulates a real
user workstation, allowing us to test actual tdh commands against realistic
scenarios without risking damage to development machines.

Architecture
------------

The live testing infrastructure consists of several components working together:

Docker Container Environment:
• Ubuntu 24.04 base image (consistent, predictable environment)
• Pre-installed dependencies: Go, Homebrew, Zsh, Bats
• Full repository mounted at /workspace
• Builds tdh binary from source for each test run
• User 'developer' with sudo access (simulates typical developer setup)

Test Framework:
• Bats (Bash Automated Testing System) for test execution
• Native JUnit XML output for CI integration
• TAP format support for debugging
• Parallel test execution capability

Scenario System:
• Each scenario represents a complete test environment
• Contains: dotfiles collection, home directory template, test files
• Setup phase creates fresh environment for each test
• Teardown phase ensures complete cleanup
• No state persists between tests

Test Suites
-----------

Tests are organized in 5 progressive suites that build in complexity:

Suite 1: Single Power-ups
• Tests each power-up in isolation (symlink, shell_profile, path, etc.)
• Verifies basic functionality works correctly
• Foundation for more complex tests

Suite 2: Multiple Power-ups, Single Pack
• Tests interactions between power-ups in the same pack
• Verifies combined deployments work correctly

Suite 3: Multiple Power-ups, Multiple Packs  
• Tests cross-pack interactions
• Verifies pack ordering and dependencies

Suite 4: Edge Cases
• Tests error conditions and boundary cases
• Verifies graceful handling of conflicts

Suite 5: Complex Scenarios
• Tests real-world usage patterns
• Verifies performance with many files

See live-testing/scenarios/test-plan.txt for detailed test specifications.

Assertion Libraries
-------------------

The test system provides power-up specific assertion libraries that abstract
implementation details and make tests readable:

assertions.sh:
• assert_symlink_deployed() - Verifies symlink chain is correct
• assert_file_exists() - Basic file system checks
• assert_env_set() - Environment variable validation

assertions_shell.sh:
• assert_shell_profile_sourced() - Checks shell initialization
• assert_profile_in_init() - Verifies tdh init.sh entries

assertions_path.sh:
• assert_path_deployed() - Verifies bin directory deployment
• assert_executable_available() - Checks PATH accessibility

assertions_install.sh:
• assert_install_script_executed() - Verifies script execution
• assert_install_artifact_exists() - Checks created files

assertions_homebrew.sh:
• assert_brewfile_processed() - Verifies Homebrew execution
• assert_brew_package_installed() - Checks package installation

Using these assertions is critical for maintainable tests. They handle the
complexity of verifying power-up behavior and provide clear error messages
when tests fail.

Running Tests
-------------

From the project root:

Run all tests with JUnit output (for CI):
    ./scripts/run-live-tests

Run all tests with pretty output (for humans):
    ./scripts/run-live-tests-pretty

Run a specific suite:
    ./scripts/run-live-tests live-testing/scenarios/suite-1/**/*.bats

Run a single test file:
    ./scripts/run-live-tests live-testing/scenarios/suite-1/symlink/tests/*.bats

Run with specific Bats options:
    ./scripts/run-live-tests --tap              # TAP format output
    ./scripts/run-live-tests --filter "deploy"  # Run tests matching pattern

Building and Managing Containers
--------------------------------

Build the test container:
    ./live-testing/scripts/build-images.sh

Build only the base image:
    ./live-testing/scripts/build-images.sh base

Launch interactive container for debugging:
    ./scripts/run-dev-container

Run arbitrary commands in container:
    ./scripts/run-dev-container ./scripts/test

Debugging Failed Tests
----------------------

When tests fail, the system automatically provides:

• Complete file tree of test directories
• Recent tdh log entries  
• Environment variable dump
• Symlink mapping for deployed files

Debug helpers available in tests:

debug_state() - Dumps complete system state
debug_symlinks() - Shows all symlinks in HOME
debug_shell_integration() - Shows shell profile state

To debug a specific failing test:

1. Run the single test to reproduce:
   ./scripts/run-live-tests path/to/failing/test.bats

2. Add debug_state calls in the test:
   @test "my failing test" {
       setup_test_env "$BATS_TEST_DIRNAME/../.."
       debug_state  # See state before action
       tdh deploy
       debug_state  # See state after action
       assert_something
   }

3. Launch interactive container to investigate:
   ./scripts/run-dev-container
   # Inside container:
   cd /workspace
   bats live-testing/scenarios/path/to/test.bats

CI Integration
--------------

The live tests run in GitHub Actions via:
• Workflow: .github/workflows/live-tests.yml
• Triggered manually or by workflow_dispatch
• Outputs JUnit XML for test reporting
• Publishes test results as GitHub checks

The same container and test infrastructure runs both locally and in CI,
ensuring consistent behavior across environments.

Performance Considerations
--------------------------

• Full test suite runs in ~5-7 minutes
• Container build is cached in CI (2-3 minutes)
• Each test has isolated environment (adds ~1s overhead)
• Parallel execution not recommended (file system conflicts)

Best Practices
--------------

When writing new tests:

1. Always use assertion libraries instead of raw checks
2. Place tests in appropriate suite based on complexity
3. Include both positive and negative test cases
4. Use meaningful test names that describe the scenario
5. Clean up any generated files in teardown
6. Add debug output for complex test logic
7. Keep individual tests focused and fast

When debugging:

1. Run failing test in isolation first
2. Use debug helpers to understand state
3. Check tdh logs for error details
4. Verify scenario files are correct
5. Test manually in interactive container
6. Add additional assertions to narrow down issue

Maintenance
-----------

The live testing system requires occasional maintenance:

• Update container base image for security patches
• Upgrade Bats when new versions are available
• Add new assertion helpers as power-ups evolve
• Prune old scenarios that no longer apply
• Monitor test execution time and optimize slow tests

See live-testing/README.txt for quick reference and directory structure.